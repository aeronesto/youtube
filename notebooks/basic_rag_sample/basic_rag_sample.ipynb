{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Knowledge Base (the index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain langchain-openai langchain-community faiss-cpu tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.vectorstores import VectorStoreRetriever\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader(\"./transcript.txt\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 200,\n",
    "    chunk_overlap=0,\n",
    "    length_function=len,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = OpenAIEmbeddings()\n",
    "index = FAISS.from_documents(docs, embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"What is RAG?\",\n",
    "    \"What are the limitations in retrieval?\"\n",
    "    \"What are the benefits of using RAG?\"\n",
    "    \"What is a RAG pipeline?\"\n",
    "    \"What happens if I ask an irrelevant question to my RAG pipeline?\"\n",
    "]\n",
    "\n",
    "query = queries[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Similary Search in Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_documents = index.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './transcript.txt'}, page_content=\"In this video, I am going to give a general overview of RAG, that's Retrieval Augmented Generation.\"),\n",
       " Document(metadata={'source': './transcript.txt'}, page_content='I will also go over some of the use cases for implementing a RAG pipeline into chatting with your documents.\\nSo you can think of RAG as an advanced search engine for searching through your documents.'),\n",
       " Document(metadata={'source': './transcript.txt'}, page_content='This is your indexing component of your rag pipeline.\\nThis is your retriever component of your rag pipeline.\\nAnd then you have your generator.\\nThese are the three components to your rag pipeline.'),\n",
       " Document(metadata={'source': './transcript.txt'}, page_content=\"And a RAG pipeline allows you to customize what pieces of those documents are provided into the LLM so that you can reduce your your token count that you're providing and reduce your cost.\")]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show all documents and scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_and_scores = index.similarity_search_with_score(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={'source': './transcript.txt'}, page_content=\"In this video, I am going to give a general overview of RAG, that's Retrieval Augmented Generation.\"),\n",
       "  0.2460782),\n",
       " (Document(metadata={'source': './transcript.txt'}, page_content='I will also go over some of the use cases for implementing a RAG pipeline into chatting with your documents.\\nSo you can think of RAG as an advanced search engine for searching through your documents.'),\n",
       "  0.30089223),\n",
       " (Document(metadata={'source': './transcript.txt'}, page_content='This is your indexing component of your rag pipeline.\\nThis is your retriever component of your rag pipeline.\\nAnd then you have your generator.\\nThese are the three components to your rag pipeline.'),\n",
       "  0.36235282),\n",
       " (Document(metadata={'source': './transcript.txt'}, page_content=\"And a RAG pipeline allows you to customize what pieces of those documents are provided into the LLM so that you can reduce your your token count that you're providing and reduce your cost.\"),\n",
       "  0.36365974)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_and_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.save_local(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_index = FAISS.load_local(\"faiss_index\", embedding_model, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = saved_index.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"What is RAG?\",\n",
    "    \"What are the limitations in retrieval?\"\n",
    "    \"What are the benefits of using RAG?\"\n",
    "    \"What is a RAG pipeline?\"\n",
    "    \"What happens if I ask an irrelevant question to my RAG pipeline?\"\n",
    "]\n",
    "\n",
    "query = queries[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = rag_chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n\\nThe limitations in retrieval are that it can be time-consuming and may not always provide accurate or relevant answers. The benefits of using RAG include the ability to implement more advanced components into the retrieval process, scalability, and the ability to limit the documents being searched. A RAG pipeline is a method of using RAG (Retrieval-Augmented Generation) to search through documents and provide factual answers. If you ask an irrelevant question to your RAG pipeline, it may not provide an accurate or relevant answer.'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer['result']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
